<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8" />
        <title>ConVQA</title>
        <!-- Bootstrap core CSS -->
        <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom styles for this template -->
        <link href="css/shop-item.css" rel="stylesheet">

        <title>ConVQA</title>

        <div class="container-fluid">
        <div class="jumbotron">
                <h1 class="display">Sunny and Dark Outside?! Improving Consistency in VQA Models</h1>
                <p class="lead"><a href="https://filebox.ece.vt.edu/~ray93/">Arijit Ray</a>, <a href="https://ksikka.com/">Karan Sikka</a>, 
                    <a href="https://sites.google.com/view/ajaydivakaran/">Ajay Divakaran</a>, 
                    <a href="https://www.cc.gatech.edu/~slee3191/">Stefan Lee</a>, 
                    <a href="https://www.linkedin.com/in/giedrius-tomas-burachas-8051578/">Giedrius Burachas</a></p>

        </div>
    </div>

        
    </head>

    <body>

        <div class="container-fluid">

            <div class="row">
                <div class="col">
                    <h2>Abstract</h2>
                    <div>While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. 
                    For instance, if a model answers “red” to “What color is the balloon?”, it might answer “no” if asked, “Is the balloon red?”. 
                    These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. 
                    In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. 
                    For a given observable fact in an image (e.g. the balloon’s color),  we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the  balloon red?)   
                    and  also  collect  a  human-annotated set of common-sense based consistent  QA  pairs  (e.g.  Is  the  balloon  the  same color  as  tomato  sauce?). 
                    Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). 
                    CTM automatically generates entailed (or similar-intent) questions for a source QA pair 
                    and fine-tunes the VQA model if the VQA’s answer to the entailed question is consistent to the source QA pair. 
                    We demonstrate that our CTM-based training improves the consistency of VQA models on the Con-VQA datasets 
                    and is a strong baseline for further research.
                </div>
            </div>
        </div>
        </div>

        <div class="container-fluid">
            <div class="row">
                <div class="col">
                        <div class = "card card-outline-secondary my-4">
                            <div class = "card-body">
                                <h3>Evaluate your VQA models on our <a href="CommonSense_ConVQA_consistent.json">common-sense-based consistent QA sets</a> to test the robustness of model's predictions to VQA2.0 Val2014 Questions.</h3>
                                <img src='csconvqaex.png'>

                                <h3>Evaluate your VQA models on our <a href="cleaned_Logical_ConVQA_test.json">logic-based consistent QA sets</a> to test the logical robustness of your model.</h3>
                                <img src='logicconvqaex.png'>
                            </div>
                        </div>
                </div>
            </div>
        </div>


        <div class="container-fluid">
            <div class="row">
                <div class="col">
                        <div class = "card card-outline-secondary my-4">
                            <div class = "card-body">
                                <a href="https://arxiv.org/abs/1909.04696">[Camera Ready Paper]</a>
                
<pre>
<code>@inproceedings{ray2019sunny,
  title={Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation},
  author={Ray, Arijit and Sikka, Karan and Divakaran, Ajay and Lee, Stefan and Burachas, Giedrius},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={5863--5868},
  year={2019}}
</code>
</pre>
                                    
                            </div>
                        </div>

                </div>
            </div>

        </div>


        <div class="container-fluid">

        <div class="row">
            <div class="col">
                <div class = "card card-outline-secondary my-4">
                    <div class = "card-header">
                        Dataset Preview
                    </div>
                    
                    <div class = "card-body">
                        <div><b>Logical Consistent QA's (L-ConVQA): <a href="Logical_ConVQA.zip"> json file </a></b></div>
                        <div> Each Visual Genome image-id (in case the image-id doesn't exactly match the official Visual Genome id, the image location is at 
                            <code>https://filebox.ece.vt.edu/~ray93/VGImages/images/VG_&lt;id&gt;.png</code>) has a separate json file named qas_&lt;id&gt;.json. The json file has two keys- consistent and inconsistent. 
                            'Consistent' contains list of sets of consistent QA pairs. 'Inconsistent' has a list of pairs of inconsistent QA's. 
                            We shall release bounding boxes and scene graph edges corresponding to these QA's soon.</div>
                        <p></p>
                        <div><b>AMT Worker Cleaned Logical Consistent QA's Test set (L-ConVQA Test): <a href="cleaned_Logical_ConVQA_test.json"> json file </a></b></div>
                        <div>Contains a list of cleaned sets of consistent QA pairs with the key being the Visual Genome image id. This is the data used to report results on L-ConVQA in the paper.</div>
                        <p></p>
                        <div><b>Human-annotated common-sense based consistency data (CS-ConVQA), Consistent Sets: <a href="CommonSense_ConVQA_consistent.json"> json file </a></b></div>
                        <div>Contains a dictionary with keys as the VQA image and values as a list of lists of consistent QA sets. This is the <a href="highlevel_test_split.json">split</a> of the dataset used to report results on CS-ConVQA in the paper.</div>
                        <p></p>
                        
                        <div><b>Human-annotated common-sense based consistency data (CS-ConVQA), Inconsistent Pairs: <a href="CommonSense_ConVQA_inconsistent.json"> json file </a></b></div>
                        <div>Contains a dictionary with keys as the VQA image and values as a list of pairs of inconsistent QA's. The inconsistent pairs were generated by automatically switching answers in selected human-annotated consistent QA pairs.</div>
                        
                        
                    </div>
                </div>
            </div>
            
        </div>

        </div>

    <div class="container-fluid">    
    <div class="row">
        <div class="col">
        <h2>Code, arxiv paper and more details coming soon!</h2>
    </div>
    </div>

    </div>

    <hr>

    </body>

    </html>
